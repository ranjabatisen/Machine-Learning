# -*- coding: utf-8 -*-
"""hw4_random_forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e-TTakB67Tmyn1u8Cy7hpTpvrMUwU4ep
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

#Importing data from dataset
training_data = pd.read_csv('health_train.csv').values
test_data = pd.read_csv('health_test.csv').values

X = training_data[:,0:-1]
y = training_data[:,-1].reshape(-1,1)

X_test = test_data[:,0:-1]
y_test = test_data[:,-1].reshape(-1,1)

def random_forest(ntree, nfeatures):
  y_predict_train = list()
  y_predict_test = list()
  for i in range(ntree):
    # train the tree
    dtree = DecisionTreeClassifier(criterion='gini', splitter='random', max_depth=2)
    dataset = pd.read_csv('health_train.csv')
    sampled_data = dataset.sample(frac=1, replace=False).to_numpy()
    X_sample = sampled_data[:,0:-1]
    y_sample = sampled_data[:,-1].reshape(-1,1)
    X_train_sampled = np.zeros(shape=(X.shape[0],312))
    X_train = np.zeros(shape=(X.shape[0],312))
    X_test1 = np.zeros(shape=(X.shape[0],312))
    for f in range(nfeatures):
      feature = np.random.randint(1,X.shape[1])
      X_train_sampled[f] = X_sample[feature]
      X_train[f] = X[feature]
      X_test1[f] = X_test[feature]
    dtree.fit(X_train_sampled, y_sample)
    # predict for training data
    yp_train = dtree.predict(X_train).reshape(1,-1)
    y_predict_train.append(yp_train)
    # predict for testing data
    yp_test = dtree.predict(X_test1).reshape(1,-1)
    y_predict_test.append(yp_test)
  y_predict_train = np.array(y_predict_train)
  y_predict_train = np.mean(y_predict_train, axis=0)
  y_predict_train = np.sign(y_predict_train[0]) 
  y_predict_train = np.array(y_predict_train).reshape(-1,1)
  y_predict_test = np.array(y_predict_test)
  y_predict_test = np.mean(y_predict_test, axis=0)
  y_predict_test = np.sign(y_predict_test[0]) 
  y_predict_test = np.array(y_predict_test).reshape(-1,1)
  return y_predict_train, y_predict_test

# Calculating error rate as no. of misclassified samples/ total no. of samples
def error_rate_calc(y, y_predict_class):
  misclassification = y_predict_class - y
  no_of_misclassified_samples = np.count_nonzero(misclassification)
  total_no_of_samples = len(misclassification)
  return no_of_misclassified_samples/total_no_of_samples

def main():
  #Varying feature sets
  feature_set = [50, 100, 150, 200, 250]
  no_tree = 100
  train_acc1 = list()
  test_acc1 = list()
  for f in feature_set:
    y_predict_train1, y_predict_test1 = random_forest(no_tree, f)
    train_acc1.append(1 - error_rate_calc(y, y_predict_train1))
    test_acc1.append(1 - error_rate_calc(y_test, y_predict_test1))
  #Varying estimators
  decision_trees = [10, 20, 40, 80, 100]
  feature_size = 250
  train_acc2 = list()
  test_acc2 = list()
  for d in decision_trees:
    y_predict_train2, y_predict_test2 = random_forest(d, feature_size)
    train_acc2.append(1 - error_rate_calc(y, y_predict_train2))
    test_acc2.append(1 - error_rate_calc(y_test, y_predict_test2))
  # Plot of accuracy vs feature sets
  plt.figure(1)
  plt.plot(train_acc1, label = 'Training accuracy')
  plt.plot(test_acc1, label = 'Test accuracy')
  plt.ylabel("Accuracy")
  plt.xlabel("Feature set size")
  plt.title("Training/Test accuracy vs Feature set size")
  plt.xticks(np.arange(5), feature_set)
  plt.legend()
  plt.show()
  # Plot of accuracy vs no. of estimators
  plt.figure(2)
  plt.plot(train_acc2, label = 'Training accuracy')
  plt.plot(test_acc2, label = 'Test accuracy')
  plt.ylabel("Accuracy")
  plt.xlabel("Number of estimators")
  plt.title("Training/Test accuracy vs Number of estimators")
  plt.xticks(np.arange(5), decision_trees)
  plt.legend()
  plt.show()

if __name__== "__main__":
    main()