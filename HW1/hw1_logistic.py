# -*- coding: utf-8 -*-
"""hw1_logistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/185311LayRpkz0FO5_hgpG2gqB9kyWVFE
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import io
from sklearn.metrics import classification_report

#Uploading file
#from google.colab import files
#uploaded = files.upload()

#Importing data from dataset
#X = pd.read_csv(io.StringIO(uploaded['IRISFeat.csv'].decode('utf-8'))).values
#y = pd.read_csv(io.StringIO(uploaded['IRISlabel.csv'].decode('utf-8'))).values

X = pd.read_csv("IRISFeat.csv").values
y = pd.read_csv("IRISlabel.csv").values

# Inserting a column with all 1s to matrix X
X0 = np.ones((X.shape[0], 1))
X = np.concatenate((X0, X), axis = 1)

#Cross validation function
def cross_validation():
    length_y = len(y)
    position = np.arange(length_y)
    #position = [0, 1, 2, ..... len(y)]
    #randomly shuffling the position array
    np.random.shuffle(position)
    X_shuffled = {}
    y_shuffled = {}
    for i in range(5):
        start_index= (i*length_y)//5
        end_index= ((i+1)*length_y)//5
        #Saving each of the 5 folds
        X_shuffled[i] = X[position[start_index:end_index],:]
        y_shuffled[i] = y[position[start_index:end_index],:]
    return X_shuffled, y_shuffled

#Combines the folds that will form training data (excluding the fold used for validation data)
def combine_folds(fold, itr):
    train_data = np.zeros((1,fold[itr].shape[1]))
    for i in range(5):
        if i != itr:
            train_data = np.concatenate((train_data, fold[i]), axis=0)
    train_data = np.delete(train_data, 0, axis=0)
    return train_data

#Function that takes one fold as validation data and combines the other folds to form training data
def get_next_train_valid(X_shuffled, y_shuffled, itr):
    X_valid = X_shuffled[itr]
    X_train = combine_folds(X_shuffled, itr)
    y_valid = y_shuffled[itr]
    y_train = combine_folds(y_shuffled, itr)
    return X_train, y_train, X_valid, y_valid

# Calculating sigma
def sigmoid_calculation(M):
    sigma = 1/(1+ np.exp(-M))
    return sigma

def train(X_train, y_train):
    learning_rate = 0.05
    model_weights = np.random.rand(3,1)
    costs_list = []
    m = X_train.shape[0]
    for i in range(4500):
        # Predictor
        prediction = sigmoid_calculation(np.dot(X_train,model_weights))
        # Calculating cost
        cost = (-1)*np.sum((np.dot(y_train.T, np.log(prediction))) + (np.dot((1-y_train.T), np.log(1-prediction))))
        cost = cost/m
        
        # Gradient descent to update weight
        dw = (np.dot(X_train.T, prediction-y_train))/m
        model_weights = model_weights - learning_rate * dw

        if(i % 50 == 0):
            costs_list.append(cost)
    model_intercept = model_weights[0,0]
    return model_weights, model_intercept, costs_list

# Predicting class
def predict(X_valid, model_weights, model_intercept):
    M = np.dot(X_valid, model_weights)
    y_predict_class = sigmoid_calculation(M)
    for i in range(len(y_predict_class)):
        if y_predict_class[i]>=0.5:
            y_predict_class[i]=1
        else:
            y_predict_class[i]=0
    return y_predict_class

# Calculating training/validation set error rate as no. of misclassified samples/ total no. of samples
def error_rate_calc(y, y_predict_class):
    misclassification = y_predict_class - y
    no_of_misclassified_samples = np.count_nonzero(misclassification)
    total_no_of_samples = len(misclassification)
    return no_of_misclassified_samples/total_no_of_samples

#main function
def main():
    #Calling cross validation function
    X_shuffled, y_shuffled = cross_validation()
    training_err=[]
    validation_err=[]
    for itr in range(5):
        # Getting training and validation data
        X_train, y_train, X_valid, y_valid = get_next_train_valid(X_shuffled, y_shuffled, itr)
        # Calculating weight using gradient descent and also cost
        model_weights,model_intercept, costs_list = train(X_train, y_train)
        # Testing model on training and validation data and storing predicted classification result
        y_predict_class_t = predict(X_train, model_weights, model_intercept)
        y_predict_class_v = predict(X_valid, model_weights, model_intercept)
        # Calculation training and validation error rates
        training_err.append(error_rate_calc(y_train,y_predict_class_t))
        validation_err.append(error_rate_calc(y_valid,y_predict_class_v))
        # Confusion matrix for validation set from 5-fold cross validation
        print(classification_report(y_valid,y_predict_class_v))

    # Plot of training and validation set error rates
    x = np.arange(5)
    width = 0.35
    fig, ax = plt.subplots()
    rects1 = ax.bar(x - width/2, training_err, width, label='Training error')
    rects2 = ax.bar(x + width/2, validation_err, width, label='Validation error')
    ax.set_title('Training error and Validation error')
    ax.legend()
    fig.tight_layout()
    plt.show()
    # Check convergence of cost function depending on choice of learning rate
    plt.plot(costs_list)
    plt.ylabel("Cost")
    plt.title("Cost Function")
    plt.show()

if __name__== "__main__":
    main()