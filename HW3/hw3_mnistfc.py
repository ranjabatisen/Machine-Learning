# -*- coding: utf-8 -*-
"""hw3_mnistfc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUxq3Ol1OSKJtlr7CLb7dnx58QMDJCip
"""

import torch
import torchvision
from torchvision import transforms, datasets
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt

# downloading data
train = datasets.MNIST(root='./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor()]))
test = datasets.MNIST(root='./data', train=False, download=True,transform=transforms.Compose([transforms.ToTensor()]))

# Mini batches of batch size 32
trainset = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)
testset = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)

# Fully connected neural network
class net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.fc1(x)
        # ReLu layer
        x = self.relu1(x)
        x = self.fc2(x)
        # softmax layer
        out = F.log_softmax(x, dim=1)
        return out

def train_net(fc, optimizer, train_loss, train_accuracy):
    num_epochs = 500
    for epoch in range(num_epochs):
        correct = 0
        total = 0
        for data in trainset:
            # data is every batch with X as features and y as classes
            X, y = data  
            # set gradient = 0
            optimizer.zero_grad()
            # forward pass the reshaped batch of size 28*28
            output = fc(X.view(-1,784))
            # cross entropy loss calculation
            loss = F.cross_entropy(output, y)
            # back propagation
            loss.backward()  
            # optimize parameters based on gradient/loss
            optimizer.step()
            # to calculate accuracy
            total += y.shape[0]    
            _, predict = output.max(1)
            correct += (predict == y).sum().item()
        train_loss.append(loss.item())
        # accuracy = no. of correct predictions/size of data
        train_accuracy.append(round(correct/total, 3))
    # saving trained model
    torch.save(fc, './mnist-fc.pt')

def test_net():
    # Load trained model
    fc = torch.load('./mnist-fc.pt')
    correct = 0
    total = 0
    with torch.no_grad():
        for features, classes in testset:
            out = fc(features.view(-1,784))
            for index in enumerate(out):
                class_index, pred_index = index
                if torch.argmax(pred_index) == classes[class_index]:
                    correct += 1
                total += 1
    # Testing accuracy
    print("Test accuracy: ", round(correct/total, 3))

def main():
    fc = net()
    learning_rate = 0.001
    # SGD as optimizer
    optimizer = optim.SGD(fc.parameters(), lr=learning_rate)
    train_loss = list()
    train_accuracy = list()
    # train the model
    train_net(fc, optimizer, train_loss, train_accuracy)
    # Check convergence of loss function
    plt.figure(1)
    plt.plot(train_loss)
    plt.ylabel("Training Loss")
    plt.xlabel("Epoch")
    plt.title("Loss Function")
    plt.show()
    # Accuracy plot
    plt.figure(2)
    plt.plot(train_accuracy)
    plt.ylabel("Training Accuracy")
    plt.xlabel("Epoch")
    plt.title("Accuracy")
    plt.show()
    # test the model
    test_net()

if __name__== "__main__":
    main()