# -*- coding: utf-8 -*-
"""hw5_vae.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FVyeouQhMfZygZVQbWJ7zb5-K8Zkhi64
"""

import torch
import torchvision
from torchvision import transforms, datasets
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# downloading data
train = datasets.MNIST(root='./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor()]))
test = datasets.MNIST(root='./data', train=False, download=True,transform=transforms.Compose([transforms.ToTensor()]))

# Mini batches of batch size 64
trainset = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)
testset = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)

# VAE
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(784, 400)
        self.relu1 = nn.ReLU()
        self.mu = nn.Linear(400, 20)
        self.sigma = nn.Linear(400, 20)
        self.fc2 = nn.Linear(20, 400)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(400, 784)
        self.sig1 = nn.Sigmoid()

    def forward(self, x):
        # Encoder
        x = self.fc1(x)
        x = self.relu1(x)
        mu = self.mu(x)
        sigma = self.sigma(x)
        # Finding z by using epsilon ~ N(0,1)
        epsilon = torch.randn(sigma.size(0),sigma.size(1))
        z = (epsilon*sigma) + mu
        # Decoder
        x = self.fc2(z)
        x = self.relu2(x)
        x = self.fc3(x)
        out = self.sig1(x)
        return out, mu, sigma

def training(optimizer, image_f, image_r, mu, sigma):
    optimizer.zero_grad()
    # binary cross entropy loss
    Loss = nn.BCELoss(reduction='sum')
    loss = Loss(image_f, image_r)
    # KL-divergence
    KLdiv = 0.5 * (torch.sum(1 + sigma.pow(2).log() - mu.pow(2) - sigma.pow(2)))
    loss = loss - KLdiv
    loss = loss/64
    # back propagation
    loss.backward()
    optimizer.step()
    loss1 = loss.item()*64
    return loss1

def calc_loss(vae, optimizer, data) :
    k, (image, _) = data
    # real image
    image_r = image.view(image.size(0), -1)
    # generating image
    image_f, mu, sigma = vae(image_r)
    loss = training(optimizer, image_f, image_r, mu, sigma)
    return loss

def train_vae(vae, optimizer, loss):
    for epoch in range(10):
        sum1 = 0
        for data in enumerate(trainset):
            vae_loss = calc_loss(vae, optimizer, data)
            sum1 = sum1 + vae_loss
        avg_loss = sum1/60000
        loss.append(avg_loss)
    # saving trained model
    torch.save(vae, './hw5_vae.pth')

def test_vae():
    # Load trained model
    vae = torch.load('./hw5_vae.pth')
    # 16 original test images
    test_images = next(iter(testset))[0].data[:16].view(16, -1)
    # reconstructed images from model
    new_images, mu, sigma = vae(test_images)
    # 4x4 grid plotting
    plt.figure(figsize=[6, 6])
    for j in range(4*4):
        plt.subplot(4, 4, j+1)
        plt.imshow(new_images.data[j].reshape(28,28), cmap='gray')
        frame = plt.gca()
        frame.axes.get_xaxis().set_visible(False)
        frame.axes.get_yaxis().set_visible(False)
    plt.subplots_adjust(wspace =0, hspace=0, top=0.6)
    plt.show()

def main():
    vae = VAE()
    # Adam as optimizer
    optimizer = optim.Adam(vae.parameters(), lr=0.0002)
    loss = list()
    # training
    train_vae(vae, optimizer, loss)
    plt.plot(loss)
    plt.ylabel("Loss")
    plt.xlabel("Epochs")
    plt.title("Loss vs Epochs")
    plt.show()
    # test the model
    test_vae()

if __name__== "__main__":
    main()